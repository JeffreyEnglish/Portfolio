{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "test_essays = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Load pre-trained model\n",
    "tokenizer_file = f\"model/trained_tokenizer\"\n",
    "model_file = f\"model/trained_lm\"\n",
    "config = AutoConfig.from_pretrained(model_file)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_file, local_files_only=True)\n",
    "\n",
    "# Tokenize\n",
    "def get_tokens(text, tokenizer, device, max_length):\n",
    "    tokens = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\") # Get tokens\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()} # Send to device\n",
    "    return tokens\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_file, force_download=False)\n",
    "\n",
    "test_data = Dataset.from_pandas(test_essays , preserve_index=False).with_format(\"torch\")\n",
    "max_length = config.max_length\n",
    "test_data = test_data.map(lambda x: {\"tokens\": get_tokens(x[\"full_text\"], tokenizer, device, max_length)}).remove_columns(\"full_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions\n",
    "all_predictions = np.zeros(len(test_data))\n",
    "test_dataloader = DataLoader(test_data, batch_size=12, shuffle=False)\n",
    "\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    for i,batch in enumerate(test_dataloader):\n",
    "        input_ids = batch['tokens']['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['tokens']['attention_mask'].squeeze(1).to(device)\n",
    "        token_type_ids = batch['tokens']['token_type_ids'].squeeze(1).to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        predictions = outputs['logits'].squeeze(1)\n",
    "\n",
    "        # Transfer predictions to CPU if needed and detach\n",
    "        predictions = predictions.cpu().detach().numpy()\n",
    "        start_idx = i * test_dataloader.batch_size\n",
    "        end_idx = start_idx + len(predictions)\n",
    "        all_predictions[start_idx:end_idx] = predictions\n",
    "               \n",
    "# Clear CUDA cache to free up memory\n",
    "# This is helpful when using a notebook for inference because memory doesn't always clear nicely\n",
    "if device != 'cpu':\n",
    "    del input_ids, attention_mask, token_type_ids, outputs, predictions\n",
    "    torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
