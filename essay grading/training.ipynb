{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mean_squared_error, cohen_kappa_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, AddedToken\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_essays = pd.read_csv('data/train.csv')\n",
    "\n",
    "n_samples = 200 # Used for quick testing, set to -1 to use all samples\n",
    "if n_samples > 0:\n",
    "    train_essays = train_essays.sample(n_samples)\n",
    "    \n",
    "# Parse out escape characters\n",
    "train_essays['full_text'] = train_essays['full_text'].str.replace(\"\\xa0\",\"\")\n",
    "\n",
    "# Split train and validation sets\n",
    "# The validation set is only used to see if the model is overfitting during the training phase\n",
    "t_size = 100\n",
    "training_samples, validation_samples = train_test_split(train_essays, test_size=t_size, random_state=1, stratify=train_essays['score'])\n",
    "training_samples.reset_index(drop=True, inplace=True)\n",
    "validation_samples.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model, tokenizer, and tf-idf transformers\n",
    "\n",
    "# Define the base model\n",
    "# Deberta is the current SOTA encoder-only model. Here we use the xsmall version for speed, but you can use base or large for better results\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "base_model = 'microsoft/deberta-v3-xsmall' \n",
    "\n",
    "# Load the tokenizer\n",
    "# We add some new tokens that are not in the base set that are helpful\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, force_download=False)\n",
    "tokenizer.add_tokens([AddedToken(\"\\n\", normalized=False)])\n",
    "tokenizer.add_tokens([AddedToken(\" \"*2, normalized=False)])\n",
    "\n",
    "# Helper function to convert a text into a token tensor\n",
    "def get_tokens(text, tokenizer, device, max_length):\n",
    "    tokens = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\") # Get tokens\n",
    "    tokens = {k: v.to(device) for k, v in tokens.items()} # Send to device\n",
    "    return tokens\n",
    "\n",
    "# Create datasets using the Datasets library\n",
    "# This allows easier loading & feature engineering later on\n",
    "train_data = Dataset.from_pandas(training_samples, preserve_index=False).with_format(\"torch\")\n",
    "validation_data = Dataset.from_pandas(validation_samples, preserve_index=False).with_format(\"torch\")\n",
    "\n",
    "# Create tf-idf embeddings\n",
    "# These will be used to evaluate the lexical similarity between essays\n",
    "tf = TfidfVectorizer(max_df=0.5, min_df=0.05)\n",
    "tf.fit(training_samples['full_text'])\n",
    "\n",
    "# Get tokens and embeddings\n",
    "# We change the maximum token length to 1024 (from 512) to avoid cutting off the longer essays\n",
    "max_length = 1024 \n",
    "train_data = train_data.map(lambda x: {\"tokens\": get_tokens(x[\"full_text\"], tokenizer, device, max_length),\n",
    "                                       \"sparse_embedding\": tf.transform([x['full_text']]).toarray()[0]})\n",
    "validation_data = validation_data.map(lambda x: {\"tokens\": get_tokens(x[\"full_text\"], tokenizer, device, max_length),\n",
    "                                       \"sparse_embedding\": tf.transform([x['full_text']]).toarray()[0]})\n",
    "train_data.add_faiss_index(column='sparse_embedding', index_name='lexical')\n",
    "\n",
    "# Helper function a neighbour for each example - used in contrastive learning\n",
    "def get_neighbour_index(embedding):\n",
    "    neighbour_id = train_data.get_nearest_examples('lexical', embedding.numpy())[1]['essay_id'][1]\n",
    "    neighbour_index = int(training_samples.loc[training_samples['essay_id'] == neighbour_id].index[0])\n",
    "    return neighbour_index\n",
    "\n",
    "# Build a dataset of contrastive examples\n",
    "contrastive_data = train_data.map(lambda x: {\"neighbour_index\": get_neighbour_index(x['sparse_embedding'])})\n",
    "contrastive_data = contrastive_data.map(lambda x: {\"neighbour_tokens\": contrastive_data[int(x['neighbour_index'])]['tokens'],\n",
    "                                       \"delta\": x['score'] - contrastive_data[int(x['neighbour_index'])]['score']})\n",
    "\n",
    "# Batch size is dependent on how much memory you have available and what size of model you are using\n",
    "# The contrastive loader loads two essays per sample so it's batch size should be half the others\n",
    "train_dataloader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "contrastive_dataloader = DataLoader(contrastive_data, batch_size=8, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "config = AutoConfig.from_pretrained(base_model)\n",
    "config.num_labels = 1\n",
    "config.max_length = max_length ## 95% percentile is 787 tokens, 99% percentile is 1026\n",
    "config.max_position_embeddings = config.max_length\n",
    "config.attention_probs_dropout_prob = 0.0 # We remove dropout because it has been shown to perform poorly in regression\n",
    "config.hidden_dropout_prob = 0.0 \n",
    "model = AutoModelForSequenceClassification.from_config(config)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_supervised(train_dataloader, validation_dataloader, optimizer):\n",
    "    start = time.time()\n",
    "    epoch_loss = 0\n",
    "    for i,batch in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['tokens']['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask = batch['tokens']['attention_mask'].squeeze(1).to(device)\n",
    "        token_type_ids = batch['tokens']['token_type_ids'].squeeze(1).to(device)\n",
    "        labels = batch['score'].float().to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        predictions = outputs['logits'].squeeze(1)\n",
    "        loss = torch.nn.MSELoss(reduction='sum')(predictions, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        n_validation_samples = len(validation_dataloader.dataset)\n",
    "        all_predictions = np.zeros(n_validation_samples)\n",
    "        for i,batch in enumerate(validation_dataloader):\n",
    "            input_ids = batch['tokens']['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask = batch['tokens']['attention_mask'].squeeze(1).to(device)\n",
    "            token_type_ids = batch['tokens']['token_type_ids'].squeeze(1).to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            predictions = outputs['logits'].squeeze(1)\n",
    "            # Transfer predictions to CPU and detach\n",
    "            predictions = predictions.cpu().detach().numpy()\n",
    "            start_idx = i * validation_dataloader.batch_size\n",
    "            end_idx = start_idx + len(predictions)\n",
    "            all_predictions[start_idx:end_idx] = predictions\n",
    "               \n",
    "        # Clear CUDA cache to free up memory\n",
    "        del input_ids, attention_mask, token_type_ids, outputs, predictions\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "        # Calculate metrics\n",
    "        test_loss = mean_squared_error(validation_data['score'], all_predictions)\n",
    "        qwk = cohen_kappa_score(validation_data['score'], all_predictions.clip(1,6).round(0), weights='quadratic')\n",
    "        print(f\"Labelled - Duration {(time.time()-start):.1f} - Train Loss {(epoch_loss/n_validation_samples):.3f} - Test Loss {test_loss:.3f} - QWK {qwk:.3f} - Prediction Range {all_predictions.min():.3f}-{all_predictions.max():.3f}\")\n",
    "        \n",
    "# Training loop for contrastive loss\n",
    "def train_epoch_contrastive(contrastive_dataloader, validation_dataloader, optimizer, weight):\n",
    "    start = time.time()\n",
    "    epoch_loss = 0\n",
    "    for i,batch in enumerate(contrastive_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids_A = batch['tokens']['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask_A = batch['tokens']['attention_mask'].squeeze(1).to(device)\n",
    "        token_type_ids_A = batch['tokens']['token_type_ids'].squeeze(1).to(device)\n",
    "\n",
    "        input_ids_B = batch['neighbour_tokens']['input_ids'].squeeze(1).to(device)\n",
    "        attention_mask_B = batch['neighbour_tokens']['attention_mask'].squeeze(1).to(device)\n",
    "        token_type_ids_B = batch['neighbour_tokens']['token_type_ids'].squeeze(1).to(device)        \n",
    "        \n",
    "        delta = batch['delta'].float().to(device)\n",
    "        label_A = batch['score'].float().to(device)\n",
    "        \n",
    "        outputs_A = model(input_ids_A, attention_mask=attention_mask_A, token_type_ids=token_type_ids_A)\n",
    "        outputs_B = model(input_ids_B, attention_mask=attention_mask_B, token_type_ids=token_type_ids_B)\n",
    "        \n",
    "        predictions_A = outputs_A['logits'].squeeze(1)\n",
    "        predictions_B = outputs_B['logits'].squeeze(1)\n",
    "        contrastive_predictions = predictions_A - predictions_B\n",
    "        \n",
    "        target = delta.sign()\n",
    "        contrastive_loss = torch.nn.MSELoss()(contrastive_predictions, target)\n",
    "        \n",
    "        loss_A = torch.nn.MSELoss(reduction='sum')(predictions_A, label_A)\n",
    "        loss = weight*contrastive_loss + (1-weight)*loss_A\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        n_validation_samples = len(validation_dataloader.dataset)\n",
    "        all_predictions = np.zeros(n_validation_samples)\n",
    "        for i,batch in enumerate(validation_dataloader):\n",
    "            input_ids = batch['tokens']['input_ids'].squeeze(1).to(device)\n",
    "            attention_mask = batch['tokens']['attention_mask'].squeeze(1).to(device)\n",
    "            token_type_ids = batch['tokens']['token_type_ids'].squeeze(1).to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "            predictions = outputs['logits'].squeeze(1)\n",
    "            # Transfer predictions to CPU and detach\n",
    "            predictions = predictions.cpu().detach().numpy()\n",
    "            start_idx = i * validation_dataloader.batch_size\n",
    "            end_idx = start_idx + len(predictions)\n",
    "            all_predictions[start_idx:end_idx] = predictions\n",
    "        \n",
    "        # Clear CUDA cache to free up memory\n",
    "        del input_ids, attention_mask, token_type_ids, outputs, predictions\n",
    "        torch.cuda.empty_cache()\n",
    "            \n",
    "        # Calculate metrics\n",
    "        test_loss = mean_squared_error(validation_data['score'], all_predictions)\n",
    "        qwk = cohen_kappa_score(validation_data['score'], all_predictions.clip(1,6).round(0), weights='quadratic')\n",
    "        print(f\"Contrastive - Duration {(time.time()-start):.1f} - Train Loss {(epoch_loss/n_validation_samples):.3f} - Test Loss {test_loss:.3f} - QWK {qwk:.3f} - Prediction Range {all_predictions.min():.3f}-{all_predictions.max():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "for _ in range(1):\n",
    "    train_epoch_supervised(train_dataloader, validation_dataloader, optimizer)\n",
    "    scheduler.step()\n",
    "for _ in range(4):\n",
    "    train_epoch_contrastive(contrastive_dataloader, validation_dataloader, optimizer, weight=0.5)\n",
    "    scheduler.step()\n",
    "for _ in range(2):\n",
    "    train_epoch_supervised(train_dataloader, validation_dataloader, optimizer)\n",
    "    scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
